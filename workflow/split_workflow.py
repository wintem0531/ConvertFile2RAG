"""PDF æ–‡æ¡£åˆ†é¡µå¤„ç†å·¥ä½œæµ

è¯¥å·¥ä½œæµå¤„ç† PDF æ–‡æ¡£çš„æ¯ä¸€é¡µï¼Œæå–å¤§å›¾åƒå’Œæ–‡æœ¬å—ï¼Œ
å¹¶æŒ‰ç…§æŒ‡å®šçš„ç›®å½•ç»“æ„ç»„ç»‡è¾“å‡ºç»“æœã€‚
"""

import hashlib
import json
import sys
import time
from pathlib import Path
from typing import Optional

import cv2
import numpy as np

# è·å–é¡¹ç›®æ ¹ç›®å½•è·¯å¾„å¹¶æ·»åŠ åˆ° Python è·¯å¾„
PROJECT_ROOT = Path(__file__).parent.parent
if str(PROJECT_ROOT) not in sys.path:
    sys.path.insert(0, str(PROJECT_ROOT))

from modes.mineru import mineru_util  # noqa: E402
from modes.imageTool import image_service  # noqa: E402
from modes.ocrTool import ocr_service  # noqa: E402


def convert_box_to_rect(box: list) -> tuple[int, int, int, int]:
    """å°†OCRè¿”å›çš„boxæ ¼å¼è½¬æ¢ä¸ºçŸ©å½¢æ¡†æ ¼å¼"""
    x_coords = [point[0] for point in box]
    y_coords = [point[1] for point in box]
    x_min = int(min(x_coords))
    y_min = int(min(y_coords))
    x_max = int(max(x_coords))
    y_max = int(max(y_coords))
    return (x_min, y_min, x_max, y_max)


def sort_boxes_left_to_right_top_to_bottom(boxes: list[list[list[float]]]) -> list[list[list[float]]]:
    """å¯¹æ£€æµ‹æ¡†è¿›è¡Œæ’åºï¼šä»å·¦åˆ°å³ã€ä»ä¸Šåˆ°ä¸‹"""
    box_info = []
    for box in boxes:
        rect_box = convert_box_to_rect(box)
        x_min, y_min, x_max, y_max = rect_box
        center_x = (x_min + x_max) / 2
        center_y = (y_min + y_max) / 2
        box_info.append((box, center_x, center_y, y_min))

    # è®¡ç®—å¹³å‡æ¡†é«˜åº¦
    if box_info:
        avg_height = np.mean([convert_box_to_rect(box)[3] - convert_box_to_rect(box)[1] for box, _, _, _ in box_info])
        row_tolerance = avg_height * 0.5
    else:
        row_tolerance = 50

    # æŒ‰è¡Œåˆ†ç»„
    rows = []
    sorted_box_info = sorted(box_info, key=lambda x: (x[3], x[1]))
    current_row = []
    current_row_y = None

    for box, center_x, center_y, y_min in sorted_box_info:
        if current_row_y is None or abs(y_min - current_row_y) <= row_tolerance:
            current_row.append((box, center_x, center_y, y_min))
            if current_row_y is None:
                current_row_y = y_min
        else:
            if current_row:
                current_row.sort(key=lambda x: x[1])
                rows.append([item[0] for item in current_row])
            current_row = [(box, center_x, center_y, y_min)]
            current_row_y = y_min

    if current_row:
        current_row.sort(key=lambda x: x[1])
        rows.append([item[0] for item in current_row])

    sorted_boxes = []
    for row in rows:
        sorted_boxes.extend(row)

    return sorted_boxes


def process_text_box_with_ocr(
    image: np.ndarray, ocr_instance, output_dir: Path, box_idx: int, image_hashes: dict[str, str]
) -> tuple[str, list[str]]:
    """å¤„ç†æ–‡æœ¬æ¡†å›¾åƒï¼Œè¿›è¡ŒOCRè¯†åˆ«"""
    text_parts = []
    image_placeholders = []

    # ä¿å­˜å›¾åƒåˆ°ä¸´æ—¶æ–‡ä»¶ä»¥ä¾¿è¿›è¡ŒOCRæ£€æµ‹
    import tempfile

    with tempfile.NamedTemporaryFile(suffix=".png", delete=False) as tmp_file:
        cv2.imwrite(tmp_file.name, image)
        tmp_path = Path(tmp_file.name)

    try:
        # è¿›è¡Œæ–‡å­—æ£€æµ‹
        boxes, _, _ = ocr_instance.detect_only(tmp_path)

        if not boxes:
            # æ²¡æœ‰æ£€æµ‹åˆ°æ–‡å­—ï¼Œä¿å­˜æ•´ä¸ªå›¾åƒ
            image_bytes = cv2.imencode(".png", image)[1].tobytes()
            image_hash = hashlib.md5(image_bytes).hexdigest()
            image_path = output_dir / f"{image_hash}.png"
            cv2.imwrite(str(image_path), image)
            image_hashes[image_hash] = str(image_path)
            image_placeholders.append(f"{{{image_hash}.png}}")
            return "", image_placeholders

        # å¯¹æ£€æµ‹æ¡†æ’åº
        sorted_boxes = sort_boxes_left_to_right_top_to_bottom(boxes)

        for idx, box in enumerate(sorted_boxes):
            # è£åˆ‡æ–‡æœ¬åŒºåŸŸ
            rect_box = convert_box_to_rect(box)
            x_min, y_min, x_max, y_max = rect_box

            # ç¡®ä¿åæ ‡åœ¨å›¾åƒèŒƒå›´å†…
            h, w = image.shape[:2]
            x_min = max(0, min(x_min, w - 1))
            y_min = max(0, min(y_min, h - 1))
            x_max = max(x_min + 1, min(x_max, w))
            y_max = max(y_min + 1, min(y_max, h))

            cropped_image = image[y_min:y_max, x_min:x_max]

            if cropped_image.size == 0:
                continue

            # è¿›è¡ŒOCRè¯†åˆ«
            text, confidence, _ = ocr_instance.recognize_text_only(cropped_image)

            # åˆ¤æ–­æ˜¯å¦ä¸ºå›¾åƒï¼ˆä½ç½®ä¿¡åº¦æˆ–æ— æ–‡å­—ï¼‰
            if not text or not text.strip() or confidence < 0.5:
                # ä¿å­˜ä¸ºå›¾åƒ
                image_bytes = cv2.imencode(".png", cropped_image)[1].tobytes()
                image_hash = hashlib.md5(image_bytes).hexdigest()
                image_path = output_dir / f"{image_hash}.png"
                cv2.imwrite(str(image_path), cropped_image)
                image_hashes[image_hash] = str(image_path)
                image_placeholders.append(f"{{{image_hash}.png}}")
            else:
                # ä¿å­˜æ–‡æœ¬
                text_parts.append(text.strip())

    finally:
        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        if tmp_path.exists():
            tmp_path.unlink()

    return " ".join(text_parts), image_placeholders


def process_pdf_page_workflow(
    pdf_input: str | Path | bytes,
    output_dir: str | Path,
    start_page: int = 1,
    end_page: int | None = None,
    min_pixels: int = 10000,
    lang: str = "ch",
    backend: str = "vlm-mlx-engine",
    **kwargs,
) -> dict[str, any]:
    """å¤„ç† PDF æ–‡æ¡£çš„å·¥ä½œæµï¼ŒæŒ‰é¡µé¢ç»„ç»‡è¾“å‡º

    Args:
        pdf_input: PDF è¾“å…¥ï¼ˆæ–‡ä»¶è·¯å¾„æˆ–å­—èŠ‚æ•°æ®ï¼‰
        output_dir: è¾“å‡ºç›®å½•
        start_page: å¼€å§‹é¡µç 
        end_page: ç»“æŸé¡µç 
        min_pixels: å¤§å›¾åƒæœ€å°åƒç´ æ•°
        lang: æ–‡æ¡£è¯­è¨€
        backend: MinerU åç«¯å¼•æ“

    Returns:
        Dict: å¤„ç†ç»“æœç»Ÿè®¡
    """
    start_time = time.time()
    output_dir = Path(output_dir)

    # å¤„ç†è¾“å…¥
    pdf_path = None
    pdf_bytes = None
    pdf_filename = "document"

    if isinstance(pdf_input, bytes):
        pdf_bytes = pdf_bytes
        import tempfile

        with tempfile.NamedTemporaryFile(suffix=".pdf", delete=False) as tmp_file:
            tmp_file.write(pdf_bytes)
            pdf_path = Path(tmp_file.name)
    else:
        pdf_path = Path(pdf_input)
        pdf_filename = pdf_path.stem

    if not pdf_path.exists():
        raise FileNotFoundError(f"PDF æ–‡ä»¶ä¸å­˜åœ¨: {pdf_path}")

    # åˆ›å»ºä¸»è¾“å‡ºç›®å½•
    pdf_output_dir = output_dir / pdf_filename
    pdf_output_dir.mkdir(parents=True, exist_ok=True)

    try:
        # è·å–æ€»é¡µæ•°
        import fitz

        doc = fitz.open(str(pdf_path))
        total_pages = len(doc)
        doc.close()

        if end_page is None:
            end_page = total_pages

        print(f"\n{'=' * 60}")
        print("PDF åˆ†é¡µå¤„ç†å·¥ä½œæµ")
        print(f"{'=' * 60}")
        print(f"ğŸ“„ è¾“å…¥æ–‡ä»¶: {pdf_path}")
        print(f"ğŸ“ è¾“å‡ºç›®å½•: {pdf_output_dir}")
        print(f"ğŸ“– é¡µé¢èŒƒå›´: {start_page} - {end_page}/{total_pages}")
        print(f"ğŸ–¼ï¸  å¤§å›¾åƒé˜ˆå€¼: {min_pixels} åƒç´ ")

        ocr_instance = ocr_service.get_ocr_service()
        processing_stats = {
            "total_pages": end_page - start_page + 1,
            "processed_pages": 0,
            "total_text_blocks": 0,
            "total_large_images": 0,
            "total_small_images": 0,
            "errors": [],
        }

        # é€é¡µå¤„ç†
        for page_num in range(start_page, end_page + 1):
            print(f"\nğŸ”„ å¤„ç†ç¬¬ {page_num}/{total_pages} é¡µ...")

            # åˆ›å»ºé¡µé¢è¾“å‡ºç›®å½•
            page_dir = pdf_output_dir / str(page_num)
            page_dir.mkdir(parents=True, exist_ok=True)
            images_dir = page_dir / "image"
            images_dir.mkdir(parents=True, exist_ok=True)

            # åˆ›å»ºä¸­é—´å¤„ç†ç›®å½•
            middle_dir = output_dir / f"middle_{page_num}"
            middle_dir.mkdir(parents=True, exist_ok=True)

            try:
                # æ­¥éª¤1: ä½¿ç”¨ MinerU è§£æé¡µé¢
                print("  ğŸ“‹ æ­¥éª¤1: ä½¿ç”¨ MinerU è§£æé¡µé¢...")
                content_list = mineru_util.parse_pdf_to_content_list(
                    pdf_path=pdf_path,
                    page_range=(page_num, page_num),
                    output_dir=middle_dir,
                    lang=lang,
                    backend=backend,
                    **kwargs,
                )

                # è¿‡æ»¤å†…å®¹å—ï¼Œæ’é™¤é¡µç ç­‰ä¸éœ€è¦çš„å†…å®¹
                content_blocks = []
                for item in content_list:
                    item_type = item.get("type", "")
                    # åªä¿ç•™æ–‡æœ¬ã€æ ‡é¢˜å’Œå›¾åƒ
                    if item_type in ["text", "title", "header", "image"]:
                        content_blocks.append(item)

                image_count = len([b for b in content_blocks if b.get("type") == "image"])
                print(f"    âœ“ æå–åˆ° {len(content_blocks)} ä¸ªå†…å®¹å—ï¼ˆ{image_count} ä¸ªå›¾åƒï¼‰")

                # æ”¶é›†æ‰€æœ‰æ–‡æœ¬å’Œå›¾åƒ
                result_parts = []
                image_hashes = {}  # å­˜å‚¨å›¾åƒå“ˆå¸Œåˆ°è·¯å¾„çš„æ˜ å°„
                text_box_idx = 1

                # æ­¥éª¤2: å¤„ç†æ¯ä¸ªå†…å®¹å—
                print("  ğŸ“ æ­¥éª¤2: å¤„ç†å†…å®¹å—...")
                for block_idx, block in enumerate(content_blocks):
                    block_type = block.get("type", "")

                    if block_type == "image":
                        # å¤„ç†å›¾åƒå—
                        img_path = block.get("img_path", "")
                        if img_path:
                            # content_list çš„å›¾åƒè·¯å¾„åœ¨ vlm ç›®å½•ä¸‹
                            full_img_path = middle_dir / pdf_filename / "vlm" / img_path
                            if not full_img_path.exists():
                                # å°è¯•å…¶ä»–å¯èƒ½çš„è·¯å¾„
                                full_img_path = middle_dir / "vlm" / img_path

                            if full_img_path.exists():
                                # è¯»å–å›¾åƒå¹¶æ£€æŸ¥å¤§å°
                                image = cv2.imread(str(full_img_path))
                                if image is not None:
                                    h, w = image.shape[:2]
                                    total_pixels = h * w

                                    if total_pixels >= min_pixels:
                                        # å¤§å›¾åƒï¼šç”Ÿæˆå“ˆå¸Œå¹¶ä¿å­˜
                                        print(f"    ğŸ–¼ï¸  å‘ç°å¤§å›¾åƒ: {w}x{h} ({total_pixels:,} åƒç´ )")
                                        image_bytes = cv2.imencode(".png", image)[1].tobytes()
                                        image_hash = hashlib.md5(image_bytes).hexdigest()
                                        output_img_path = images_dir / f"{image_hash}.png"
                                        cv2.imwrite(str(output_img_path), image)
                                        image_hashes[image_hash] = str(output_img_path)
                                        result_parts.append(f"{{{image_hash}.png}}")
                                        processing_stats["total_large_images"] += 1

                                        # æ‰“å°å›¾åƒè¯´æ˜ï¼ˆå¦‚æœæœ‰ï¼‰
                                        captions = block.get("image_caption", [])
                                        if captions:
                                            print(f"      è¯´æ˜: {', '.join(captions)}")
                                    else:
                                        # å°å›¾åƒï¼šä¿å­˜ä¸ºæ–‡æœ¬æ¡†å›¾åƒ
                                        print(f"    ğŸ“„ å‘ç°å°å›¾åƒ: {w}x{h} ({total_pixels:,} åƒç´ )")
                                        text_box_path = page_dir / f"text_box_{text_box_idx}.png"
                                        cv2.imwrite(str(text_box_path), image)
                                        text_box_idx += 1
                                        processing_stats["total_small_images"] += 1

                    elif block_type in ["text", "title", "header"]:
                        # å¤„ç†æ–‡æœ¬å—
                        text_content = block.get("text", "")
                        if text_content:
                            result_parts.append(text_content)
                            processing_stats["total_text_blocks"] += 1
                            print(f"    ğŸ“„ æ–‡æœ¬å—: {text_content[:50]}...")

                # æ­¥éª¤3: å¤„ç†æ–‡æœ¬æ¡†å›¾åƒ
                print("  ğŸ” æ­¥éª¤3: å¤„ç†æ–‡æœ¬æ¡†å›¾åƒ...")
                text_box_files = sorted(page_dir.glob("text_box_*.png"))

                for text_box_path in text_box_files:
                    print(f"    ğŸ“· å¤„ç†: {text_box_path.name}")
                    image = cv2.imread(str(text_box_path))

                    # ä½¿ç”¨OCRå¤„ç†æ–‡æœ¬æ¡†
                    text, image_placeholders = process_text_box_with_ocr(
                        image=image,
                        ocr_instance=ocr_instance,
                        output_dir=images_dir,
                        box_idx=int(text_box_path.stem.split("_")[-1]),
                        image_hashes=image_hashes,
                    )

                    # å°†ç»“æœæ’å…¥åˆ°é€‚å½“ä½ç½®
                    # è¿™é‡Œç®€å•è¿½åŠ ï¼Œå®é™…å¯èƒ½éœ€è¦æ ¹æ®ä½ç½®ä¿¡æ¯æ’å…¥
                    if text:
                        result_parts.append(text)
                    if image_placeholders:
                        result_parts.extend(image_placeholders)

                    # åˆ é™¤åŸå§‹æ–‡æœ¬æ¡†æ–‡ä»¶
                    text_box_path.unlink()

                # æ­¥éª¤4: ä¿å­˜æ–‡æœ¬ç»“æœ
                print("  ğŸ’¾ æ­¥éª¤4: ä¿å­˜æ–‡æœ¬ç»“æœ...")
                text_output = "\n".join(str(part) for part in result_parts)
                text_file_path = page_dir / "text.txt"
                text_file_path.write_text(text_output, encoding="utf-8")

                # æ¸…ç†ä¸­é—´ç›®å½•
                import shutil

                if middle_dir.exists():
                    shutil.rmtree(middle_dir, ignore_errors=True)

                processing_stats["processed_pages"] += 1
                print(f"  âœ… ç¬¬ {page_num} é¡µå¤„ç†å®Œæˆ")

            except Exception as e:
                error_msg = f"å¤„ç†ç¬¬ {page_num} é¡µæ—¶å‡ºé”™: {str(e)}"
                print(f"  âŒ {error_msg}")
                processing_stats["errors"].append({"page": page_num, "error": error_msg})

                # æ¸…ç†ä¸­é—´ç›®å½•
                import shutil

                if middle_dir.exists():
                    shutil.rmtree(middle_dir, ignore_errors=True)
                continue

        # ä¿å­˜æ•´ä½“ç»Ÿè®¡ä¿¡æ¯
        stats_file = pdf_output_dir / "processing_stats.json"
        with open(stats_file, "w", encoding="utf-8") as f:
            json.dump(
                {
                    "processing_info": {
                        "pdf_path": str(pdf_path),
                        "start_page": start_page,
                        "end_page": end_page,
                        "min_pixels": min_pixels,
                        "processing_time": round(time.time() - start_time, 2),
                    },
                    "statistics": processing_stats,
                },
                f,
                ensure_ascii=False,
                indent=2,
            )

        # æ‰“å°æœ€ç»ˆç»Ÿè®¡
        print(f"\n{'=' * 60}")
        print("ğŸ“Š å¤„ç†å®Œæˆç»Ÿè®¡")
        print(f"{'=' * 60}")
        print(f"ğŸ“„ å¤„ç†é¡µé¢: {processing_stats['processed_pages']}/{processing_stats['total_pages']}")
        print(f"ğŸ“ æ–‡æœ¬å—: {processing_stats['total_text_blocks']} ä¸ª")
        print(f"ğŸ–¼ï¸  å¤§å›¾åƒ: {processing_stats['total_large_images']} ä¸ª")
        print(f"ğŸ“„ å°å›¾åƒ(ä½œä¸ºæ–‡æœ¬æ¡†): {processing_stats['total_small_images']} ä¸ª")
        print(f"âŒ é”™è¯¯æ•°: {len(processing_stats['errors'])}")
        print(f"â±ï¸  æ€»è€—æ—¶: {round(time.time() - start_time, 2)} ç§’")
        print("\nğŸ“ è¾“å‡ºç»“æ„:")
        print(f"  {pdf_output_dir}/")
        print("  â”œâ”€â”€ 1/")
        print("  â”‚   â”œâ”€â”€ text.txt")
        print("  â”‚   â””â”€â”€ image/")
        print("  â”‚       â””â”€â”€ {hash}.png")
        print("  â”œâ”€â”€ 2/")
        print("  â”‚   â””â”€â”€ ...")
        print("  â””â”€â”€ processing_stats.json")

        return processing_stats

    finally:
        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        if isinstance(pdf_input, bytes) and pdf_path and pdf_path.exists():
            import os

            try:
                os.unlink(pdf_path)
            except Exception:
                pass


def main():
    """ä¸»å‡½æ•° - æ¼”ç¤ºå·¥ä½œæµ"""
    print("\nğŸš€ PDF åˆ†é¡µå¤„ç†å·¥ä½œæµæ¼”ç¤º")

    # æŸ¥æ‰¾æµ‹è¯•æ–‡ä»¶
    test_dir = PROJECT_ROOT / "test_file"
    test_files = list(test_dir.rglob("*.pdf"))[:1]

    if not test_files:
        test_files = list(test_dir.rglob("*.png"))[:1]

    if not test_files:
        print("âŒ æœªæ‰¾åˆ°æµ‹è¯•æ–‡ä»¶")
        return

    test_file = test_files[0]
    output_dir = PROJECT_ROOT / "test_file" / "split_workflow_output"

    try:
        process_pdf_page_workflow(
            pdf_input=test_file,
            output_dir=output_dir,
            start_page=25,
            end_page=25,  # åªå¤„ç†ç¬¬25é¡µ
            min_pixels=10000,  # ä½¿ç”¨é»˜è®¤é˜ˆå€¼
            lang="ch",
            backend="vlm-mlx-engine",
        )

        print("\nâœ… å·¥ä½œæµæ‰§è¡Œå®Œæˆ!")

    except Exception as e:
        print(f"\nâŒ å·¥ä½œæµæ‰§è¡Œå¤±è´¥: {e}")
        import traceback

        traceback.print_exc()


if __name__ == "__main__":
    main()
